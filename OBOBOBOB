[1mdiff --git a/bpf/cpu/cpu.bpf.c b/bpf/cpu/cpu.bpf.c[m
[1mindex 47ae84bd..f955d111 100644[m
[1m--- a/bpf/cpu/cpu.bpf.c[m
[1m+++ b/bpf/cpu/cpu.bpf.c[m
[36m@@ -13,6 +13,7 @@[m
 #include <bpf/bpf_endian.h>[m
 #include <bpf/bpf_helpers.h>[m
 #include <bpf/bpf_tracing.h>[m
[32m+[m[32m#include "shared.h"[m
 [m
 /*================================ CONSTANTS =================================*/[m
 // Programs.[m
[36m@@ -84,6 +85,11 @@[m [menum stack_walking_method {[m
   STACK_WALKING_METHOD_DWARF = 1,[m
 };[m
 [m
[32m+[m[32menum interpreter_type {[m
[32m+[m[32m  INTERPRETER_TYPE_UNDEFINED = 0,[m
[32m+[m[32m  INTERPRETER_TYPE_RUBY = 1,[m
[32m+[m[32m};[m
[32m+[m
 struct unwinder_config_t {[m
   bool filter_processes;[m
   bool verbose_logging;[m
[36m@@ -152,20 +158,6 @@[m [mtypedef struct {[m
   chunk_info_t chunks[MAX_UNWIND_TABLE_CHUNKS];[m
 } unwind_info_chunks_t;[m
 [m
[31m-// The addresses of a native stack trace.[m
[31m-typedef struct {[m
[31m-  u64 len;[m
[31m-  u64 addresses[MAX_STACK_DEPTH];[m
[31m-} stack_trace_t;[m
[31m-[m
[31m-typedef struct {[m
[31m-  int pid;[m
[31m-  int tgid;[m
[31m-  int user_stack_id;[m
[31m-  int kernel_stack_id;[m
[31m-  int user_stack_id_dwarf;[m
[31m-} stack_count_key_t;[m
[31m-[m
 // Represents an executable mapping.[m
 typedef struct {[m
   u64 load_address;[m
[36m@@ -178,21 +170,11 @@[m [mtypedef struct {[m
 // Executable mappings for a process.[m
 typedef struct {[m
   u64 is_jit_compiler;[m
[32m+[m[32m  u64 interpreter_type;[m
   u64 len;[m
   mapping_t mappings[MAX_MAPPINGS_PER_PROCESS];[m
 } process_info_t;[m
 [m
[31m-// State of unwinder such as the registers as well[m
[31m-// as internal data.[m
[31m-typedef struct {[m
[31m-  u64 ip;[m
[31m-  u64 sp;[m
[31m-  u64 bp;[m
[31m-  u32 tail_calls;[m
[31m-  stack_trace_t stack;[m
[31m-  bool unwinding_jit; // set to true during JITed unwinding; false unless mixed-mode unwinding is enabled[m
[31m-} unwind_state_t;[m
[31m-[m
 // A row in the stack unwinding table for x86_64.[m
 typedef struct __attribute__((packed)) {[m
   u64 pc;[m
[36m@@ -222,13 +204,6 @@[m [mBPF_HASH(unwind_info_chunks, u64, unwind_info_chunks_t,[m
 BPF_HASH(unwind_tables, u64, stack_unwind_table_t,[m
          5); // Table size will be updated in userspace.[m
 [m
[31m-struct {[m
[31m-  __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);[m
[31m-  __uint(max_entries, 1);[m
[31m-  __type(key, u32);[m
[31m-  __type(value, unwind_state_t);[m
[31m-} heap SEC(".maps");[m
[31m-[m
 struct {[m
   __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);[m
   __uint(max_entries, 1);[m
[36m@@ -602,8 +577,7 @@[m [mstatic __always_inline bool has_fp(u64 current_fp) {[m
 [m
 // Aggregate the given stacktrace.[m
 static __always_inline void add_stack(struct bpf_perf_event_data *ctx, u64 pid_tgid, enum stack_walking_method method, unwind_state_t *unwind_state) {[m
[31m-  u64 zero = 0;[m
[31m-  stack_count_key_t stack_key = {0};[m
[32m+[m[32m  stack_count_key_t *stack_key = &unwind_state->stack_key;[m
 [m
   // The `bpf_get_current_pid_tgid` helpers returns[m
   // `current_task->tgid << 32 | current_task->pid`, the naming can be[m
[36m@@ -614,13 +588,13 @@[m [mstatic __always_inline void add_stack(struct bpf_perf_event_data *ctx, u64 pid_t[m
   // - What we call threads IDs in user space, are PIDs in kernel space.[m
   int user_pid = pid_tgid >> 32;[m
   int user_tgid = pid_tgid;[m
[31m-  stack_key.pid = user_pid;[m
[31m-  stack_key.tgid = user_tgid;[m
[32m+[m[32m  stack_key->pid = user_pid;[m
[32m+[m[32m  stack_key->tgid = user_tgid;[m
 [m
   if (method == STACK_WALKING_METHOD_DWARF) {[m
     int stack_hash = MurmurHash2((u32 *)unwind_state->stack.addresses, MAX_STACK_DEPTH * sizeof(u64) / sizeof(u32), 0);[m
     LOG("stack hash %d", stack_hash);[m
[31m-    stack_key.user_stack_id_dwarf = stack_hash;[m
[32m+[m[32m    stack_key->user_stack_id_dwarf = stack_hash;[m
 [m
     // Insert stack.[m
     int err = bpf_map_update_elem(&dwarf_stack_traces, &stack_hash, &unwind_state->stack, BPF_ANY);[m
[36m@@ -637,7 +611,9 @@[m [mstatic __always_inline void add_stack(struct bpf_perf_event_data *ctx, u64 pid_t[m
       LOG("[warn] bpf_get_stackid user failed with %d", stack_id);[m
       return;[m
     }[m
[31m-    stack_key.user_stack_id = stack_id;[m
[32m+[m[32m    stack_key->user_stack_id = stack_id;[m
[32m+[m[32m  } else {[m
[32m+[m[32m    LOG("[error] invalid native unwinding method: %d", method);[m
   }[m
 [m
   // Get kernel stack.[m
[36m@@ -646,15 +622,29 @@[m [mstatic __always_inline void add_stack(struct bpf_perf_event_data *ctx, u64 pid_t[m
     LOG("[warn] bpf_get_stackid kernel failed with %d", kernel_stack_id);[m
     return;[m
   }[m
[31m-  stack_key.kernel_stack_id = kernel_stack_id;[m
[32m+[m[32m  stack_key->kernel_stack_id = kernel_stack_id;[m
 [m
[31m-  // Aggregate stacks.[m
[31m-  u64 *scount = bpf_map_lookup_or_try_init(&stack_counts, &stack_key, &zero);[m
[31m-  if (scount) {[m
[31m-    __sync_fetch_and_add(scount, 1);[m
[32m+[m[32m  // Continue unwinding interpreter, if any.[m
[32m+[m[32m  process_info_t *proc_info = bpf_map_lookup_elem(&process_info, &user_pid);[m
[32m+[m[32m  if (proc_info == NULL) {[m
[32m+[m[32m    LOG("[error] should never happen");[m
[32m+[m[32m    return;[m
[32m+[m[32m  }[m
[32m+[m[32m  switch (proc_info->interpreter_type) {[m
[32m+[m[32m    case INTERPRETER_TYPE_UNDEFINED:[m
[32m+[m[32m      LOG("[debug] not an interpreter");[m
[32m+[m[32m      bpf_tail_call(ctx, &programs, AGGREGATE_STACKS_PROGRAM_ID);[m
[32m+[m[32m      break;[m
[32m+[m[32m    case INTERPRETER_TYPE_RUBY:[m
[32m+[m[32m      LOG("[debug] Ruby interpreter");[m
[32m+[m[32m      bpf_tail_call(ctx, &programs, RUBY_UNWINDER_PROGRAM_ID);[m
[32m+[m[32m      break;[m
[32m+[m[32m    default:[m
[32m+[m[32m      LOG("[warn] bad interpreter value: %d", proc_info->interpreter_type);[m
[32m+[m[32m      break;[m
   }[m
 [m
[31m-  request_process_mappings(ctx, user_pid);[m
[32m+[m[32m  return;[m
 }[m
 [m
 // The unwinding machinery lives here.[m
[36m@@ -953,10 +943,8 @@[m [mint walk_user_stacktrace_impl(struct bpf_perf_event_data *ctx) {[m
 [m
     if (unwind_state->bp == 0) {[m
       LOG("======= reached main! =======");[m
[31m-      add_stack(ctx, pid_tgid, STACK_WALKING_METHOD_DWARF, unwind_state);[m
       bump_unwind_success_dwarf();[m
[31m-      // Traverse interpreter stack.[m
[31m-      bpf_tail_call(ctx, &programs, RUBY_UNWINDER_PROGRAM_ID);[m
[32m+[m[32m      add_stack(ctx, pid_tgid, STACK_WALKING_METHOD_DWARF, unwind_state);[m
     } else {[m
       int user_pid = pid_tgid;[m
       process_info_t *proc_info = bpf_map_lookup_elem(&process_info, &user_pid);[m
[36m@@ -1003,6 +991,12 @@[m [mstatic __always_inline bool set_initial_state(bpf_user_pt_regs_t *regs) {[m
   unwind_state->stack.len = 0;[m
   unwind_state->tail_calls = 0;[m
   unwind_state->unwinding_jit = false;[m
[32m+[m[32m  // Reset key.[m
[32m+[m[32m  unwind_state->stack_key.kernel_stack_id = 0;[m
[32m+[m[32m  unwind_state->stack_key.pid = 0;[m
[32m+[m[32m  unwind_state->stack_key.tgid = 0;[m
[32m+[m[32m  unwind_state->stack_key.user_stack_id = 0;[m
[32m+[m[32m  unwind_state->stack_key.user_stack_id_dwarf = 0;[m
 [m
   u64 ip = 0;[m
   u64 sp = 0;[m
[36m@@ -1112,9 +1106,7 @@[m [mint profile_cpu(struct bpf_perf_event_data *ctx) {[m
   // 2. We did not have unwind information, let's see if we can unwind with frame[m
   // pointers.[m
   if (has_fp(unwind_state->bp)) {[m
[31m-    add_stack(ctx, pid_tgid, STACK_WALKING_METHOD_FP, NULL);[m
[31m-    // Traverse interpreter stack.[m
[31m-    bpf_tail_call(ctx, &programs, RUBY_UNWINDER_PROGRAM_ID);[m
[32m+[m[32m    add_stack(ctx, pid_tgid, STACK_WALKING_METHOD_FP, unwind_state);[m
     return 0;[m
   }[m
 [m
[1mdiff --git a/bpf/cpu/rbperf.bpf.c b/bpf/cpu/rbperf.bpf.c[m
[1mindex 0ac8a04d..459fa094 100644[m
[1m--- a/bpf/cpu/rbperf.bpf.c[m
[1m+++ b/bpf/cpu/rbperf.bpf.c[m
[36m@@ -12,6 +12,8 @@[m
 #include <bpf/bpf_helpers.h>[m
 #include <bpf/bpf_tracing.h>[m
 [m
[32m+[m[32m#include "shared.h"[m
[32m+[m
 /* struct {[m
     // This map's type is a placeholder, it's dynamically set[m
     // in rbperf.rs to either perf/ring buffer depending on[m
[36m@@ -61,34 +63,6 @@[m [mstruct {[m
     __type(value, SampleState);[m
 } global_state SEC(".maps");[m
 [m
[31m-/////////////////////[m
[31m-[m
[31m-#define MAX_STACK_DEPTH 127[m
[31m-[m
[31m-[m
[31m-typedef struct {[m
[31m-  u64 len;[m
[31m-  u64 addresses[MAX_STACK_DEPTH];[m
[31m-} stack_trace_t;[m
[31m-[m
[31m-typedef struct {[m
[31m-  u64 ip;[m
[31m-  u64 sp;[m
[31m-  u64 bp;[m
[31m-  u32 tail_calls;[m
[31m-  stack_trace_t stack;[m
[31m-  bool unwinding_jit; // set to true during JITed unwinding; false unless mixed-mode unwinding is enabled[m
[31m-} unwind_state_t;[m
[31m-[m
[31m-struct {[m
[31m-  __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);[m
[31m-  __uint(max_entries, 1);[m
[31m-  __type(key, u32);[m
[31m-  __type(value, unwind_state_t);[m
[31m-} heap SEC(".maps");[m
[31m-[m
[31m-///////////////////////[m
[31m-[m
 const volatile bool verbose = false;[m
 const volatile bool use_ringbuf = false;[m
 const volatile bool enable_pid_race_detector = true;[m
[1mdiff --git a/pkg/profiler/cpu/maps.go b/pkg/profiler/cpu/maps.go[m
[1mindex 1f3971c4..5ce4db0e 100644[m
[1m--- a/pkg/profiler/cpu/maps.go[m
[1m+++ b/pkg/profiler/cpu/maps.go[m
[36m@@ -606,7 +606,6 @@[m [mfunc (m *bpfMaps) addUnwindTableForProcess(pid int, executableMappings unwind.Ex[m
 	}[m
 [m
 	// Important: the below *must* be called before setUnwindTable.[m
[31m-	// .is_jit_compiler[m
 	var isJitCompiler uint64[m
 	if executableMappings.HasJitted() {[m
 		isJitCompiler = 1[m
[36m@@ -617,8 +616,10 @@[m [mfunc (m *bpfMaps) addUnwindTableForProcess(pid int, executableMappings unwind.Ex[m
 	}[m
 [m
 	mappingInfoMemory := m.mappingInfoMemory.Slice(mappingInfoSizeBytes)[m
[31m-	// .type[m
[32m+[m	[32m// .is_jit_compiler[m
 	mappingInfoMemory.PutUint64(isJitCompiler)[m
[32m+[m	[32m// .interpreter_type[m
[32m+[m	[32mmappingInfoMemory.PutUint64(1)[m
 	// .len[m
 	mappingInfoMemory.PutUint64(uint64(len(executableMappings)))[m
 [m
